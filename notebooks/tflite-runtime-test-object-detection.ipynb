{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzsr-eFF6XlN"
      },
      "source": [
        "# TFLite runtime test from MediaPipe\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/google-coral-micro-object-detection/blob/master/notebooks/tflite-runtime-test-object-detection.ipynb)\n",
        "\n",
        "Perform post processing according to: https://github.com/google/mediapipe/blob/8609e5fae5c89bad69995e2ffeec62a9fefc0cb9/mediapipe/calculators/tensor/tensors_to_detections_calculator.cc#L787\n",
        "\n",
        "```\n",
        "Author: Shawn Hymel\n",
        "Date: December 23, 2023\n",
        "License: https://opensource.org/license/0bsd/\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "204jMBKO2De8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6CSf7Ha2GV5"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "MODEL_PATH = \"model_int8.tflite\"\n",
        "METADATA_PATH = \"metadata.json\"\n",
        "IMAGE_PATH = \"image.64eb8fd9.jpg\"\n",
        "SCORE_THRESHOLD = 0.5\n",
        "IOU_THRESHOLD = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ra-f_DS2HUC"
      },
      "outputs": [],
      "source": [
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(MODEL_PATH)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p45S1w1e2IEB"
      },
      "outputs": [],
      "source": [
        "# Get input and output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "# Get output details unique to MobileNetv2-SSD\n",
        "num_anchors = output_details[0]['shape'][1]\n",
        "num_locs = output_details[0]['shape'][2]\n",
        "num_classes = output_details[1]['shape'][2]\n",
        "\n",
        "# Get input quantization details (assume one input tensor)\n",
        "input_type = input_details[0]['dtype']\n",
        "if input_type is np.uint8:\n",
        "    input_scale = input_details[0]['quantization_parameters']['scales'][0]\n",
        "    input_zero_point = input_details[0]['quantization_parameters']['zero_points'][0]\n",
        "elif input_type is np.float32:\n",
        "    input_scale = None\n",
        "    input_zero_point = None\n",
        "else:\n",
        "    raise TypeError(\"Unsupported input tensor type\")\n",
        "\n",
        "# Get location output quantization details (assume one tensor)\n",
        "locs_type = output_details[0]['dtype']\n",
        "if locs_type is np.uint8:\n",
        "    locs_scale = output_details[0]['quantization_parameters']['scales'][0]\n",
        "    locs_zero_point = output_details[0]['quantization_parameters']['zero_points'][0]\n",
        "elif locs_type is np.float32:\n",
        "    locs_scale = None\n",
        "    locs_zero_point = None\n",
        "else:\n",
        "    raise TypeError(\"Unsupported output locations tensor type\")\n",
        "\n",
        "# Get scores output quantization details (assume one tensor)\n",
        "scores_type = output_details[1]['dtype']\n",
        "if scores_type is np.uint8:\n",
        "    scores_scale = output_details[1]['quantization_parameters']['scales'][0]\n",
        "    scores_zero_point = output_details[1]['quantization_parameters']['zero_points'][0]\n",
        "elif scores_type is np.float32:\n",
        "    scores_scale = None\n",
        "    scores_zero_point = None\n",
        "else:\n",
        "    raise TypeError(\"Unsupported output scores tensor type\")\n",
        "\n",
        "# Print details\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "print(f\"Number of anchors: {num_anchors}\")\n",
        "print(f\"Number of locations: {num_locs}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Input type: {input_type}\")\n",
        "print(f\"Input scale: {input_scale}\")\n",
        "print(f\"Input zero point: {input_zero_point}\")\n",
        "print(f\"Locations type: {locs_type}\")\n",
        "print(f\"Locations scale: {locs_scale}\")\n",
        "print(f\"Locations zero point: {locs_zero_point}\")\n",
        "print(f\"Scores type: {scores_type}\")\n",
        "print(f\"Scores scale: {scores_scale}\")\n",
        "print(f\"Scores zero point: {scores_zero_point}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28TiKjBB7loe"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the JPEG image\n",
        "input_shape = input_details[0]['shape']\n",
        "image = Image.open(IMAGE_PATH)\n",
        "image = image.resize((input_shape[1], input_shape[2]))\n",
        "\n",
        "# Normalize or quantize the image (depending on expected input type)\n",
        "if input_type is np.uint8:\n",
        "    image = np.array(image, dtype=np.uint8)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "elif input_type is np.float32:\n",
        "    image = np.array(image, dtype=np.float32)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image /= 255.0\n",
        "else:\n",
        "    raise TypeError(\"Unsupported input tensor type\")\n",
        "\n",
        "# Set the input tensor with the preprocessed image\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensors\n",
        "scores = interpreter.get_tensor(output_details[1]['index'])\n",
        "raw_boxes = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(scores.shape)\n",
        "print(raw_boxes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TY0Zne82Jro"
      },
      "outputs": [],
      "source": [
        "# Import model metadata\n",
        "with open(METADATA_PATH, 'r') as file:\n",
        "    metadata = json.load(file)\n",
        "\n",
        "# Parse metadata\n",
        "custom_metadata = metadata['subgraph_metadata'][0]['custom_metadata'][0]\n",
        "anchors = custom_metadata['data']['ssd_anchors_options']['fixed_anchors_schema']['anchors']\n",
        "num_values_per_keypoint = custom_metadata['data']['tensors_decoding_options']['num_values_per_keypoint']\n",
        "apply_exponential_on_box_size = custom_metadata['data']['tensors_decoding_options']['apply_exponential_on_box_size']\n",
        "x_scale = custom_metadata['data']['tensors_decoding_options']['x_scale']\n",
        "y_scale = custom_metadata['data']['tensors_decoding_options']['y_scale']\n",
        "w_scale = custom_metadata['data']['tensors_decoding_options']['w_scale']\n",
        "h_scale = custom_metadata['data']['tensors_decoding_options']['h_scale']\n",
        "\n",
        "# Print metadata\n",
        "print(f\"Number of anchors: {len(anchors)}\")\n",
        "print(f\"Values per keypoint: {num_values_per_keypoint}\")\n",
        "print(f\"Apply exponential scaling: {apply_exponential_on_box_size}\")\n",
        "print(f\"X scale: {x_scale}\")\n",
        "print(f\"Y scale: {y_scale}\")\n",
        "print(f\"Width scale: {w_scale}\")\n",
        "print(f\"Height scale: {h_scale}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2k-eitK4VI1"
      },
      "outputs": [],
      "source": [
        "# Change threshold to quantized value if needed\n",
        "if scores_type is np.uint8:\n",
        "    score_threshold = np.round(SCORE_THRESHOLD / scores_scale) + scores_zero_point\n",
        "elif scores_type is np.float32:\n",
        "    score_threshold = SCORE_THRESHOLD\n",
        "else:\n",
        "    raise TypeError(\"Unsupported scores output tensor type\")\n",
        "\n",
        "# Go through all anchor points and create a list of bounding boxes\n",
        "bboxes = []\n",
        "for i in range(num_anchors):\n",
        "    for c, s in enumerate(scores[0, i, :]):\n",
        "\n",
        "        # Only keep boxes above a particular score threshold\n",
        "        if c > 0 and s >= score_threshold:\n",
        "\n",
        "            # Assume raw box output tensor is given as YXHW (MobileNet SSD)\n",
        "            # TODO: Handle different output tensor formats\n",
        "            y_center = raw_boxes[0, i, 0]\n",
        "            x_center = raw_boxes[0, i, 1]\n",
        "            h = raw_boxes[0, i, 2]\n",
        "            w = raw_boxes[0, i, 3]\n",
        "\n",
        "            # De-quantize if needed\n",
        "            if locs_type is np.uint8:\n",
        "                y_center = (y_center - locs_zero_point) * locs_scale\n",
        "                x_center = (x_center - locs_zero_point) * locs_scale\n",
        "                h = (h - locs_zero_point) * locs_scale\n",
        "                w = (w - locs_zero_point) * locs_scale\n",
        "            elif locs_type is np.float32:\n",
        "                pass\n",
        "            else:\n",
        "                raise TypeError(\"Unsupported location output type type\")\n",
        "\n",
        "            # Scale\n",
        "            x_center = x_center / x_scale * anchors[i]['width'] + anchors[i]['x_center']\n",
        "            y_center = y_center / y_scale * anchors[i]['height'] + anchors[i]['y_center']\n",
        "            if apply_exponential_on_box_size:\n",
        "                w = math.exp(w / w_scale) * anchors[i]['width']\n",
        "                h = math.exp(h / h_scale) * anchors[i]['height']\n",
        "            else:\n",
        "                w = w / w_scale * anchors[i]['width']\n",
        "                h = h / h_scale * anchors[i]['height']\n",
        "\n",
        "            # Assume no keypoint detections in MobileNet SSD\n",
        "            # TODO: handle keypoints\n",
        "\n",
        "            # Determine min/max coordinates\n",
        "            x_min = x_center - w / 2.0\n",
        "            y_min = y_center - h / 2.0\n",
        "            x_max = x_center + w / 2.0\n",
        "            y_max = y_center + h / 2.0\n",
        "\n",
        "\n",
        "            # Clamp values\n",
        "            x_min = max(0, min(1.0, x_min))\n",
        "            y_min = max(0, min(1.0, y_min))\n",
        "            x_max = max(0, min(1.0, x_max))\n",
        "            y_max = max(0, min(1.0, y_max))\n",
        "\n",
        "            # Append bounding box\n",
        "            bboxes.append([c, s, x_min, y_min, x_max, y_max])\n",
        "\n",
        "# Print boxes\n",
        "for bbox in bboxes:\n",
        "    print(bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvN9Y3VbW2Lm"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box1, box2):\n",
        "\n",
        "    # Coordinates of the intersection box\n",
        "    x_min_inter = max(box1[2], box2[2])\n",
        "    y_min_inter = max(box1[3], box2[3])\n",
        "    x_max_inter = min(box1[4], box2[4])\n",
        "    y_max_inter = min(box1[5], box2[5])\n",
        "\n",
        "    # Width and height of the intersection box\n",
        "    width_inter = max(0, x_max_inter - x_min_inter)\n",
        "    height_inter = max(0, y_max_inter - y_min_inter)\n",
        "\n",
        "    # Area of intersection\n",
        "    area_inter = width_inter * height_inter\n",
        "\n",
        "    # Areas of the input boxes\n",
        "    area_box1 = (box1[4] - box1[2]) * (box1[5] - box1[3])\n",
        "    area_box2 = (box2[4] - box2[2]) * (box2[5] - box2[3])\n",
        "\n",
        "    # Area of union\n",
        "    area_union = area_box1 + area_box2 - area_inter\n",
        "\n",
        "    # Intersection over Union\n",
        "    iou = area_inter / area_union if area_union != 0 else 0\n",
        "\n",
        "    return iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v65SUFZqR6GR"
      },
      "outputs": [],
      "source": [
        "# Non-maximum suppression\n",
        "# https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\n",
        "bboxes_nms = []\n",
        "for i in range(len(bboxes)):\n",
        "    discard = False\n",
        "    for j in range(len(bboxes)):\n",
        "\n",
        "        # Do not compare bboxes with different classes\n",
        "        if bboxes[i][0] != bboxes[j][0]:\n",
        "            continue\n",
        "\n",
        "        # If j box is higher score than i and above IOU threshold, mark to discard\n",
        "        iou = calculate_iou(bboxes[i], bboxes[j])\n",
        "        if iou > IOU_THRESHOLD:\n",
        "            if bboxes[j][1] > bboxes[i][1]:\n",
        "                discard = True\n",
        "\n",
        "    # Only keep highest scoring box\n",
        "    if not discard:\n",
        "        bboxes_nms.append(bboxes[i])\n",
        "\n",
        "# If quantized output, de-quantize the scores\n",
        "if scores_type is np.uint8:\n",
        "    for bbox in bboxes_nms:\n",
        "        bbox[1] = (bbox[1] - scores_zero_point) * scores_scale\n",
        "\n",
        "# Print boxes\n",
        "for bbox in bboxes_nms:\n",
        "    print(bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-XeXMvZWdDs"
      },
      "outputs": [],
      "source": [
        "# Load the image\n",
        "image = plt.imread(IMAGE_PATH)\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(1)\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(image)\n",
        "\n",
        "# Iterate through bounding boxes in bbox_list and draw them\n",
        "for bbox in bboxes_nms:\n",
        "    class_id, score, xmin, ymin, xmax, ymax = bbox\n",
        "    left = int(xmin * image.shape[1] + 0.5)\n",
        "    top = int(ymin * image.shape[0] + 0.5)\n",
        "    width = int((xmax - xmin) * image.shape[1] + 0.5)\n",
        "    height = int((ymax - ymin) * image.shape[0] + 0.5)\n",
        "\n",
        "    # Create a Rectangle patch\n",
        "    rect = patches.Rectangle((left, top), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "\n",
        "    # Add the patch to the Axes\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# Show the image with bounding boxes\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFx80_bCcvpY"
      },
      "outputs": [],
      "source": [
        "# Print bounding box information\n",
        "for bbox in bboxes_nms:\n",
        "    class_id, score, xmin, ymin, xmax, ymax = bbox\n",
        "    left = int(xmin * image.shape[1] + 0.5)\n",
        "    top = int(ymin * image.shape[0] + 0.5)\n",
        "    width = int((xmax - xmin) * image.shape[1] + 0.5)\n",
        "    height = int((ymax - ymin) * image.shape[0] + 0.5)\n",
        "\n",
        "    print(f\"class: {class_id}, score: {score}, left: {left}, top: {top}, width: {width}, height: {height}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmuZTOh11LFW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
