{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FyPuNcVXq8s9"
      },
      "source": [
        "# MediaPipe Object Detection Learning\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/google-coral-micro-object-detection/blob/master/notebooks/mediapipe-object-detection-learning.ipynb)\n",
        "\n",
        "```\n",
        "Original authors: MediaPipeline (Google)\n",
        "Modified by: Shawn Hymel\n",
        "Date: December 16, 2023\n",
        "```\n",
        "\n",
        "Use transfer learning with Google MediaPipe to build a custom object detection model. Based on the example code from https://developers.google.com/mediapipe/solutions/customization/object_detector.\n",
        "\n",
        "> **Note:** This script has been verified with TensorFlow v2.15.0.\n",
        "\n",
        "To use this script, upload your dataset in [Pascal VOC format](http://host.robots.ox.ac.uk/pascal/VOC/) in an archive named *dataset.zip*. You can use a labeling tool like [labelImg](https://github.com/HumanSignal/labelImg) or [Make Sense](https://www.makesense.ai/) to create bounding box annotations in the Pascal VOC format.\n",
        "\n",
        "\n",
        "Your data should be in the following format. Note that the directory names \"Annotations\" and \"images\" must be exactly as shown (with the capital 'A' and lowercase 'i').\n",
        "\n",
        "```\n",
        "dataset.zip\n",
        "├── Annotations/\n",
        "│   ├── image.01.xml\n",
        "│   ├── image.02.xml\n",
        "│   ├── ...\n",
        "└── images/\n",
        "    ├── image.01.jpg\n",
        "    ├── image.02.jpg\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "Run through all the cells. Adjust the hyperparameters (`hparams`) as needed to achieve the desired accuracy. Ideally, you want your average precision (AP) to be greater than 90% to get a useful object detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1JOF-3nPnbl"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vOxSHjPb1S"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Vwn9W6rQju"
      },
      "outputs": [],
      "source": [
        "# Install MediaPipe and Edge TPU compiler\n",
        "!python --version\n",
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker\n",
        "! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "! sudo apt-get update\n",
        "! sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6i8DzRmWP2N"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from mediapipe_model_maker import object_detector, quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDCndaU1O_Tk"
      },
      "outputs": [],
      "source": [
        "# Check TensorFlow version\n",
        "print(tf.__version__)\n",
        "assert tf.__version__.startswith('2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdZb8awuq7gx"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "BASE_PATH = \".\"\n",
        "DATASET_ZIP_PATH = os.path.join(BASE_PATH, \"dataset.zip\")\n",
        "DATASET_PATH = os.path.join(BASE_PATH, \"dataset/\")\n",
        "TRAIN_SPLIT = 0.8\n",
        "EXPORT_PATH = os.path.join(BASE_PATH, \"exported_models/\")\n",
        "TFLITE_FLOAT32_NAME = \"model.tflite\"\n",
        "TFLITE_INT8_NAME = \"model_int8.tflite\"\n",
        "METADATA_PATH = os.path.join(EXPORT_PATH, \"metadata.json\")\n",
        "METADATA_H_NAME = \"metadata.hpp\"\n",
        "METADATA_H_PATH = os.path.join(EXPORT_PATH, METADATA_H_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTrJMYmctlMQ"
      },
      "source": [
        "## Create dataset\n",
        "\n",
        "Load and prepare the dataset for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CNE-9xBr7Qs"
      },
      "outputs": [],
      "source": [
        "# Unzip dataset\n",
        "!rm -rf {DATASET_PATH}\n",
        "!unzip -q {DATASET_ZIP_PATH} -d {DATASET_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE0saod5sKtn"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = object_detector.Dataset.from_pascal_voc_folder(DATASET_PATH)\n",
        "\n",
        "# Split the dataset into separate training and validation sets\n",
        "train_data, validation_data = data.split(TRAIN_SPLIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf6EwReH-3Jy"
      },
      "source": [
        "## Train object detection model\n",
        "\n",
        "Use transfer learning to retrain a model. Gather more/better data and adjust the hyperparameters (`hparams`) to ideally obtain a `total_loss` of less than 0.1 and an average precision (AP) of greater than 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX8RXjnauguy"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and specify hyperparameters\n",
        "spec = object_detector.SupportedModels.MOBILENET_V2_I320\n",
        "hparams = object_detector.HParams(\n",
        "    learning_rate = 0.3,\n",
        "    batch_size=8,\n",
        "    epochs=50,\n",
        "    export_dir=EXPORT_PATH,\n",
        ")\n",
        "options = object_detector.ObjectDetectorOptions(\n",
        "    supported_model=spec,\n",
        "    hparams=hparams,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLTzpJYFAIt-"
      },
      "outputs": [],
      "source": [
        "# Retrain model\n",
        "model = object_detector.ObjectDetector.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzmHFaQCAlfO"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance\n",
        "loss, coco_metrics = model.evaluate(\n",
        "    validation_data,\n",
        "    batch_size=4,\n",
        ")\n",
        "print(f\"Validation loss: {loss}\")\n",
        "print(f\"Validation metrics: {coco_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3BoPclpBFUN"
      },
      "source": [
        "## Export model\n",
        "\n",
        "Save the model in three different formats:\n",
        "\n",
        " 1. 32-bit floating point TensorFlow Lite (TFLite)\n",
        " 2. 8-bit integer quantized TFLite\n",
        " 3. TPU compiled and quantized TFLite|\n",
        "\n",
        "Additionally, save the metadata (anchor box information) in a .h file that a resource-constrained device can recalculate the anchor boxes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dttq5K2bA_Pn"
      },
      "outputs": [],
      "source": [
        "# Export 32-bit float model\n",
        "model.export_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry2xxZm4CUIP"
      },
      "outputs": [],
      "source": [
        "# Perform post-training quantization (8-bit integer) and save quantized model\n",
        "quantization_config = quantization.QuantizationConfig.for_int8(\n",
        "    representative_data=validation_data,\n",
        ")\n",
        "model.restore_float_ckpt()\n",
        "model.export_model(\n",
        "    model_name=TFLITE_INT8_NAME,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mljgQv4sHFkj"
      },
      "outputs": [],
      "source": [
        "# Compile the model for Edge TPU\n",
        "!edgetpu_compiler -s -o {EXPORT_PATH} {os.path.join(EXPORT_PATH, TFLITE_INT8_NAME)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HtGzw_rgeoJ"
      },
      "outputs": [],
      "source": [
        "# Import model metadata\n",
        "with open(METADATA_PATH, 'r') as file:\n",
        "    metadata = json.load(file)\n",
        "\n",
        "# Parse metadata\n",
        "custom_metadata = metadata['subgraph_metadata'][0]['custom_metadata'][0]\n",
        "anchors = custom_metadata['data']['ssd_anchors_options']['fixed_anchors_schema']['anchors']\n",
        "num_values_per_keypoint = custom_metadata['data']['tensors_decoding_options']['num_values_per_keypoint']\n",
        "apply_exponential_on_box_size = custom_metadata['data']['tensors_decoding_options']['apply_exponential_on_box_size']\n",
        "x_scale = custom_metadata['data']['tensors_decoding_options']['x_scale']\n",
        "y_scale = custom_metadata['data']['tensors_decoding_options']['y_scale']\n",
        "w_scale = custom_metadata['data']['tensors_decoding_options']['w_scale']\n",
        "h_scale = custom_metadata['data']['tensors_decoding_options']['h_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To1IvviFsYls"
      },
      "outputs": [],
      "source": [
        "# Figure out when the resets (sectors) occur, the x/y increases, and width/height of anchors\n",
        "reset_idxs = []\n",
        "y_strides = []\n",
        "x_strides = []\n",
        "widths_per_section = []\n",
        "widths = []\n",
        "heights_per_section = []\n",
        "heights = []\n",
        "reset_flag = True\n",
        "x_stride_flag = True\n",
        "width_flag = True\n",
        "\n",
        "# Go through all the anchors\n",
        "num_anchors = len(anchors)\n",
        "for i in range(num_anchors):\n",
        "\n",
        "    # Store the first index\n",
        "    if i == 0:\n",
        "        reset_idxs.append(i)\n",
        "\n",
        "    # Only measure strides on not 0 indexes\n",
        "    else:\n",
        "\n",
        "        # New section: reset flags\n",
        "        if anchors[i]['y_center'] < anchors[i - 1]['y_center']:\n",
        "            reset_idxs.append(i)\n",
        "            reset_flag = True\n",
        "            x_stride_flag = True\n",
        "            width_flag = True\n",
        "\n",
        "        # Measure Y increase (stride)\n",
        "        if reset_flag:\n",
        "            if anchors[i]['y_center'] > anchors[i - 1]['y_center']:\n",
        "                y_inc = anchors[i]['y_center'] - anchors[i - 1]['y_center']\n",
        "                y_strides.append(round(y_inc, 5))\n",
        "                reset_flag = False\n",
        "\n",
        "        # Measure X increase (stride)\n",
        "        if x_stride_flag:\n",
        "            if anchors[i]['x_center'] > anchors[i - 1]['x_center']:\n",
        "                x_inc = anchors[i]['x_center'] - anchors[i - 1]['x_center']\n",
        "                x_strides.append(round(x_inc, 5))\n",
        "                x_stride_flag = False\n",
        "\n",
        "    # Record widths and heights of the anchor boxes\n",
        "    if width_flag:\n",
        "        if i != 0 and anchors[i]['x_center'] > anchors[i - 1]['x_center']:\n",
        "            widths.append(widths_per_section)\n",
        "            widths_per_section = []\n",
        "            heights.append(heights_per_section)\n",
        "            heights_per_section = []\n",
        "            width_flag = False\n",
        "        else:\n",
        "            width = anchors[i]['width']\n",
        "            widths_per_section.append(round(width, 5))\n",
        "            height = anchors[i]['height']\n",
        "            heights_per_section.append(round(height, 5))\n",
        "\n",
        "# Calculate the number of sectors\n",
        "num_sectors = len(reset_idxs)\n",
        "\n",
        "# Calculate the number of anchors per coordinate\n",
        "num_anchors_per_coord = len(widths[0])\n",
        "\n",
        "# Calculate the number of Xs in each Y\n",
        "num_xs_per_y = []\n",
        "for sector in range(num_sectors):\n",
        "    num_xs_per_y.append(int(1.0 / x_strides[sector] * num_anchors_per_coord))\n",
        "\n",
        "print(f\"Number of anchors {num_anchors}\")\n",
        "print(f\"Number of sectors: {num_sectors}\")\n",
        "print(f\"Number of anchors per coordinate: {num_anchors_per_coord}\")\n",
        "print(f\"Reset indexes: {reset_idxs}\")\n",
        "print(f\"Number of Xs per Y: {num_xs_per_y}\")\n",
        "print(f\"X strides: {x_strides}\")\n",
        "print(f\"Y strides: {y_strides}\")\n",
        "print(\"Widths:\")\n",
        "for wps in widths:\n",
        "    print(wps)\n",
        "print(\"Heights:\")\n",
        "for hps in heights:\n",
        "    print(hps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKyxi0L9hB-5"
      },
      "outputs": [],
      "source": [
        "# Generate header file for metadata information\n",
        "h_str = f\"\"\"\\\n",
        "// Filename: {METADATA_H_NAME}\n",
        "\n",
        "#ifndef METADATA_HPP\n",
        "#define METADATA_HPP\n",
        "\n",
        "namespace metadata {{\n",
        "    constexpr unsigned int num_anchors = {num_anchors};\n",
        "    constexpr int apply_exp_scaling = {1 if apply_exponential_on_box_size else 0};\n",
        "    constexpr float x_scale = {x_scale};\n",
        "    constexpr float y_scale = {y_scale};\n",
        "    constexpr float w_scale = {w_scale};\n",
        "    constexpr float h_scale = {h_scale};\n",
        "    constexpr unsigned int num_sectors = {num_sectors};\n",
        "    constexpr unsigned int num_anchors_per_coord = {num_anchors_per_coord};\n",
        "\"\"\"\n",
        "\n",
        "# Print reset indexes\n",
        "h_str += \"    constexpr unsigned int reset_idxs[] = {\\r\\n\"\n",
        "h_str += \"        \"\n",
        "for i in range(num_sectors):\n",
        "    h_str += f\"{reset_idxs[i]}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \", \"\n",
        "h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Print the number of X values for each Y value\n",
        "h_str += \"    constexpr unsigned int num_xs_per_y[] = {\\r\\n\"\n",
        "h_str += \"        \"\n",
        "for i in range(num_sectors):\n",
        "    h_str += f\"{num_xs_per_y[i]}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \", \"\n",
        "h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Print the X strides\n",
        "h_str += \"    constexpr float x_strides[] = {\\r\\n\"\n",
        "h_str += \"        \"\n",
        "for i in range(num_sectors):\n",
        "    h_str += f\"{x_strides[i]}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \", \"\n",
        "h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Print the Y strides\n",
        "h_str += \"    constexpr float y_strides[] = {\\r\\n\"\n",
        "h_str += \"        \"\n",
        "for i in range(num_sectors):\n",
        "    h_str += f\"{y_strides[i]}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \", \"\n",
        "h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Print the anchor widths for each section\n",
        "h_str += f\"    constexpr float widths[{num_sectors}][{len(widths[0])}] = {{\\r\\n\"\n",
        "for i in range(num_sectors):\n",
        "    h_str += \"        {\"\n",
        "    for j in range(len(widths[0])):\n",
        "        h_str += f\"{widths[i][j]}\"\n",
        "        if j < len(widths[0]) - 1:\n",
        "            h_str += \", \"\n",
        "    h_str += \"}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \",\"\n",
        "    h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Print the anchor heights for each section\n",
        "h_str += f\"    constexpr float heights[{num_sectors}][{len(heights[0])}] = {{\\r\\n\"\n",
        "for i in range(num_sectors):\n",
        "    h_str += \"        {\"\n",
        "    for j in range(len(heights[0])):\n",
        "        h_str += f\"{heights[i][j]}\"\n",
        "        if j < len(heights[0]) - 1:\n",
        "            h_str += \", \"\n",
        "    h_str += \"}\"\n",
        "    if i < num_sectors - 1:\n",
        "        h_str += \",\"\n",
        "    h_str += \"\\r\\n\"\n",
        "h_str += \"    };\\r\\n\"\n",
        "\n",
        "# Close header file\n",
        "h_str += \"\"\"\\\n",
        "}\n",
        "\n",
        "#endif // METADATA_HPP\n",
        "\"\"\"\n",
        "\n",
        "# write to .h file\n",
        "with open(METADATA_H_PATH, 'w') as file:\n",
        "    file.write(h_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_ieJWMdLLof"
      },
      "outputs": [],
      "source": [
        "# Zip exported models\n",
        "zip_name = os.path.normpath(EXPORT_PATH).split(os.sep)[-1] + \".zip\"\n",
        "zip_path = os.path.join(BASE_PATH, zip_name)\n",
        "!zip -q -r {zip_path} {EXPORT_PATH}/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_5cU4ZOGGY"
      },
      "outputs": [],
      "source": [
        "# Download exported models\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8T3qKUMjYkK"
      },
      "outputs": [],
      "source": [
        "!zip -q -r exported_models.zip {os.path.join(EXPORT_PATH, \"*\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHwO7UCdXtMZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
